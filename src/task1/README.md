## ğŸ“Œ Task 1 Instructions: Dead Sea Scrolls

**All steps can be run using the provided `Makefile` via the `make` command.**  
- Use `make task_1` to run the full pipeline.  
- Use `make test_task_1` for a quick test run with smaller parameters.  
- Use `make reset_task_1` to remove all generated folders and reset the task environment.

All scripts for this task are located in `src/task1/scripts`.

---

### ğŸ—‚ï¸ Setup

1. **Download Hebrew characters**  
   Download the `monkbrill/` folder and place it at the root of the repository. This contains the character images used for training.

2. **Clean the character data**  
   Run the cleaning script to generate a new `monkbrill_clean/` folder with cleaned characters.

3. **Download and binarize noise maps**  
   - Download the noise maps from the provided Google Drive link.  
   - Place them inside a `noise_maps/` folder at the root.  
   - Run the binarization script to generate `noise_maps/binarized/`.

4. **Install ImageMorph tool**  
   Download from: [https://github.com/GrHound/imagemorph.c](https://github.com/GrHound/imagemorph.c)  
   This is used during the data augmentation step for character morphing.

---

### ğŸ“ Text Translation (Optional)

Use `translation.py` to translate Aesopâ€™s fables or other `.txt` files.  
Translated `.txt` files are already available in the `text_files/` folder.  
If you add your own files, you must specify whether they are for training or validation in the YOLO step.

---

### ğŸ–¼ï¸ Synthetic Data Generation

Run the following scripts to create synthetic scroll images:
- `create_random_scrolls.py`: Generates random scrolls using the cleaned characters and noise maps.
- `create_text_scrolls.py`: Generates scrolls from Hebrew `.txt` files.

These scripts rely on the cleaned character set (`monkbrill_clean/`) and binarized noise maps (`noise_maps/binarized/`).

---

### âœ‚ï¸ Line Segmentation

Use `line_segmentation.py` to segment the scrolls into individual lines.  
This creates a `segmented_scrolls/` folder, which is used for training YOLO.

---

### ğŸ§  Train the Detector

Run `train_detector.py` to train the YOLO model on the segmented scrolls using your dataset and configuration (`src/hebrew.yaml`, etc.).

---

### ğŸ” Predict & Visualize

- Use the prediction script to apply the trained YOLO model to new scroll images.
- Use `visualise_bounding_boxes.py` to overlay bounding boxes from label files onto images for inspection.

---

### âš™ï¸ Makefile Commands Summary

| Command             | Description                                                                 |
|---------------------|-----------------------------------------------------------------------------|
| `make task_1`       | Runs the full Task 1 pipeline with default parameters.                      |
| `make test_task_1`  | Runs a small test pipeline (1 augment per char, 50/20 scrolls, 1 epoch).    |
| `make reset_task_1` | Deletes `augmented_chars`, `dataset`, and `noise_maps/binarized/` folders. |


---

## ğŸ“ Project Final Layout

- `monkbrill/` â€” Original Hebrew character dataset (to be downloaded manually)
- `monkbrill_clean/` â€” Cleaned characters (generated by script)
- `noise_maps/` â€” Folder for noise maps (to be downloaded)
  - `binarized/` â€” Binarized version of noise maps (generated by script)
- `text_files/` â€” Translated Hebrew text files (already provided)

- `generated_scrolls/` â€” Folder with the generated scrolls from text (eg. bible_train.txt)
- `synthetic_scrolls/` â€” Folder with the synthetic random text scrolls
- `segmented_scrolls/` â€” Contains segmented line images (generated by script)

- `src/task1/scripts/` â€” All scripts related to Task 1
- `Makefile` â€” For setting up environment, installing dependencies, and cleaning
- `requirements.txt` â€” Code requirements

---

