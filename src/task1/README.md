## üìå Task 1 Instructions: Dead Sea Scrolls

**All steps can be run using the provided `Makefile` via the `make` command.**  
- Use `make task_1` to run the full pipeline.  
- Use `make test_task_1` for a quick test run with smaller parameters.  
- Use `make reset_task_1` to remove all generated folders and reset the task environment.

All scripts for this task are located in `src/task1/scripts`.

---

### üóÇÔ∏è Setup

1. **Download Hebrew characters**  
   Download the `monkbrill/` folder and place it at the root of the repository. This contains the character images used for training.

2. **Clean the character data**  
   Run `data_cleaning.py` to generate a new `monkbrill_clean/` folder with cleaned characters.

3. **Download and binarize noise maps**  
   - Download the noise maps from the provided Google Drive link.  
   - Place them inside a `noise_maps/` folder at the root.  
   - Run `noise_maps_binarizing.py` to get `noise_maps/binarized/`.

4. **Install ImageMorph tool**  
   Download from: [https://github.com/GrHound/imagemorph.c](https://github.com/GrHound/imagemorph.c)  
   This is used during the data augmentation step for character morphing.

---

### Text Translation (Optional)

Use `translation.py` to translate Aesop‚Äôs fables or other `.txt` files.  
Translated `.txt` files are already available in the `text_files/` folder.  
If you add your own files, you must implicitly modify the scroll generation script and specify whether they are for training or validation and place them in the correct directory.

---

### Synthetic Data Generation

Run the following script to create synthetic scroll images:
- `scrolls_generator.py`: Generates random scrolls using the cleaned characters and noise maps, involving both random n-grams and existing text.

The script relies on the cleaned character set (`monkbrill_clean/`) and binarized noise maps (`noise_maps/binarized/`).

The scrolls are stored inside `dataset/synthetic_scrolls_text` and `dataset/synthetic_scrolls_random`.
---

### Line Segmentation

Use `line_segmentation.py` to segment the scrolls into individual lines.  

The scrolls are stored inside `dataset/segmented_scrolls_random` and `dataset/segmented_scrolls_text`. These are used to train YOLO (specified in hebrew.yaml and hebrew_ft.yaml respectively).

---

### Train the Detector

Run `train_detector.py` to train the YOLO model on the segmented scrolls using your dataset and configuration (`src/hebrew.yaml`, `src/hebrew_ft.yaml`).

---

### Predict & Visualize

- Use the `make_prediction.py` prediction script to apply the trained YOLO model to new scroll image and visualize while printing the predicted hebrew text.
- Use `yolo_recognizer.py` to execute the whole inference pipeline (line segmentation & detection). The predictions are stored in the output directory as .txt files. 

---

### ‚öôÔ∏è Makefile Commands Summary

| Command             | Description                                                                 |
|---------------------|-----------------------------------------------------------------------------|
| `make task_1`       | Runs the full Task 1 pipeline with default parameters.                      |
| `make test_task_1`  | Runs a small test pipeline (1 augment per char, 50/20 scrolls, 1 epoch).    |
| `make reset_task_1` | Deletes `augmented_chars`, `dataset`, and `noise_maps/binarized/` folders. |


---

## üìÅ Project Final Layout

- `monkbrill/` ‚Äî Original Hebrew character dataset (to be downloaded manually)
- `monkbrill_clean/` ‚Äî Cleaned characters (generated by script)
- `noise_maps/` ‚Äî Folder for noise maps (to be downloaded)
  - `binarized/` ‚Äî Binarized version of noise maps (generated by script)
- `text_files/` ‚Äî Translated Hebrew text files (already provided)

- `generated_scrolls/` ‚Äî Folder with the generated scrolls from text (eg. bible_train.txt)
- `synthetic_scrolls/` ‚Äî Folder with the synthetic random text scrolls
- `segmented_scrolls/` ‚Äî Contains segmented line images (generated by script)

- `src/task1/scripts/` ‚Äî All scripts related to Task 1
- `Makefile` ‚Äî For setting up environment, installing dependencies, and cleaning
- `requirements.txt` ‚Äî Code requirements

---

## Limitations

## üö´ Limitations

This project version of Task 1 has a few known limitations, mostly due to time constraints and data constraints:

1. **Missing Tsadi Variants in Generated Data**  
   The file `ngram_frequencies_withNames.csv` includes ancient Hebrew n-gram statistics, where the character *tsadi* appears. However, our `hebrew.yaml` used for YOLO training distinguishes only *tsadi-medial* and *tsadi-final*, not a generic *tsadi*. As a result, *tsadi* is never generated in scrolls, and the model has never seen or been trained to predict it. This inconsistency was discovered late in development and fixing it would have required regenerating thousands of scrolls, which normally takes several hours, and of couse it was infeasible before the submission deadline.

2. **Use of Modern Hebrew Texts**  
   The `bible.txt` and translated Aesop‚Äôs fables are likely written in modern Hebrew. While we hoped for some linguistic overlap with ancient Hebrew, there are notable differences. A practical consequence is for instance that the  *mem-medial* character never appears in these texts and thus is missing from the training data, which is visible in the confusion matrix of the final model. While YOLO is primarily a visual model and does not heavily rely on long-range context, the linguistic mismatch introduces some label shift.

3. **Limited Scroll Realism**  
   Our synthetic scrolls include noise for realism, but the distribution of this noise differs noticeably from real Dead Sea Scrolls. Since the test set includes real fragments, this domain gap likely affects final performance. Future work could involve domain adaptation or noise simulation better aligned with test scrolls.

4. **Data Coverage and Scarcity**  
   While we generated 10,000 scrolls for training and validation, this may still not be sufficient. Further improvement could involve generating more random n-gram scrolls or attempting to mine ancient Hebrew texts from the web in 'text format' (if available), though it is uncertain if such data exists and requires further exploration.

5. **Post-YOLO Correction Model Not Implemented**  
   Ideally, misclassifications from YOLO detection could be corrected using a sequential model. We did not implement this due to scope and time constraints.
